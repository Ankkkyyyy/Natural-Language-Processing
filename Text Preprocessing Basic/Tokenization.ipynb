{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Using the split function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'Norway']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenization \n",
    "\n",
    "sent1 = \"I am going to Norway\"\n",
    "sent1.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to Norwway for Northern Lights',\n",
       " \" Let's hope the trip to be great\",\n",
       " '']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence level tokenization \n",
    "\n",
    "sent2 = \"I am going to Norwway for Northern Lights. Let's hope the trip to be great.\"\n",
    "sent2.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'Norway!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Problem with Split Function \n",
    "\n",
    "sent3 = \"I am going to Norway!\"\n",
    "sent3.split()\n",
    "# Now in output you can see it is taking Norway! differently, so whenever the new word Norway will come \n",
    "# it will take Norway and Norway! both as different words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where do think i should go? i have 3 day holiday']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent4 = 'Where do think i should go? i have 3 day holiday'\n",
    "sent4.split('.')\n",
    "\n",
    "# yaha full stop option  nahi hai \n",
    "# so isliye seperate nahi hoparaha, so yeahsbh problems hoti in split function "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.)  Regular Expression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split se thoda better hai Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'going', 'to', 'Norway']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "sentr1 = \"I am going to Norway\"\n",
    "tokens = re.findall(\"[\\w]+\",sentr1)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem, ipsum dolor sit amet consectetur adipisicing elit',\n",
       " ' \\nPlaceat distinctio nemo sit animi aliquid libero id excepturi possimus voluptatem mollitia \\nratione quae eos numquam architecto, repellat assumenda debitis neque',\n",
       " ' \\nEst magnam dolor dolorem adipisci sequi soluta non veniam provident repellat',\n",
       " '']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem, ipsum dolor sit amet consectetur adipisicing elit. \n",
    "Placeat distinctio nemo sit animi aliquid libero id excepturi possimus voluptatem mollitia \n",
    "ratione quae eos numquam architecto, repellat assumenda debitis neque. \n",
    "Est magnam dolor dolorem adipisci sequi soluta non veniam provident repellat.\"\"\"\n",
    "sentences  = re.compile(\"[.!?]\").split(text)\n",
    "sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) NLTK\n",
    "\n",
    "better than regular expressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'vist', 'Norway', '!']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordText = 'I am going to vist Norway!'\n",
    "word_tokenize(wordText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem, ipsum dolor sit amet consectetur adipisicing elit.',\n",
       " 'Placeat distinctio nemo sit animi aliquid libero id excepturi possimus voluptatem mollitia \\nratione quae eos numquam architecto, repellat assumenda debitis neque.',\n",
       " 'Est magnam dolor dolorem adipisci sequi soluta non veniam provident repellat.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem, ipsum dolor sit amet consectetur adipisicing elit. \n",
    "Placeat distinctio nemo sit animi aliquid libero id excepturi possimus voluptatem mollitia \n",
    "ratione quae eos numquam architecto, repellat assumenda debitis neque. \n",
    "Est magnam dolor dolorem adipisci sequi soluta non veniam provident repellat.\"\"\"\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1  = 'I have Ph.D in A.I'\n",
    "text2 = \"We're here to help! mail us at ankitxtowod@gmail.com\"\n",
    "text3 = \"A 5km ride cost $10.50\"\n",
    "text4 = 'I am going to vist Norway!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'Ph.D', 'in', 'A.I']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " \"'re\",\n",
       " 'here',\n",
       " 'to',\n",
       " 'help',\n",
       " '!',\n",
       " 'mail',\n",
       " 'us',\n",
       " 'at',\n",
       " 'ankitxtowod',\n",
       " '@',\n",
       " 'gmail.com']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see even NLTK bhi atakta hai fail hota hai   \n",
    "as usne dekho  Email Id kobhi Break kardiya hai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', '5km', 'ride', 'cost', '$', '10.50']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5km prblm hai yaha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.) Spacy \n",
    "\n",
    "better NLP library to deal with tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(text1)\n",
    "doc2 = nlp(text2)\n",
    "doc3 = nlp(text3)\n",
    "doc4 = nlp(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "have\n",
      "Ph\n",
      ".\n",
      "D\n",
      "in\n",
      "A.I\n"
     ]
    }
   ],
   "source": [
    "for token in doc1 :\n",
    "    print(token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ph  .d it is taking it as seperate \n",
    "\n",
    "here Spacy Fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "'re\n",
      "here\n",
      "to\n",
      "help\n",
      "!\n",
      "mail\n",
      "us\n",
      "at\n",
      "ankitxtowod@gmail.com\n"
     ]
    }
   ],
   "source": [
    "for token in doc2 :\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "5\n",
      "km\n",
      "ride\n",
      "cost\n",
      "$\n",
      "10.50\n"
     ]
    }
   ],
   "source": [
    "for token in doc3:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "going\n",
      "to\n",
      "vist\n",
      "Norway\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in doc4:\n",
    "    print(token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy bss Ph.D vale case mai Fail hua hai \n",
    "\n",
    "### Spacy mostly best result deta hai in tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
